{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, MLDatasets, ImageCore, StatsBase\n",
    "using MLDatasets: MNIST\n",
    "using Flux: train!, update!\n",
    "using Flux: onehot, throttle, crossentropy, relu, sigmoid\n",
    "using StatsBase: sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux: Data.DataLoader\n",
    "using Flux: onehotbatch, onecold, crossentropy\n",
    "using Flux: @epochs\n",
    "using Statistics\n",
    "using MLDatasets\n",
    "\n",
    "# load full training set\n",
    "train_x, train_y = MNIST.traindata();\n",
    "\n",
    "train_x_vec = [vec(train_x[:, :, i]) for i = 1:60000];\n",
    "train_y_hot = [onehot(train_y[i], 0:9) for i = 1:60000];\n",
    "\n",
    "# load full test set\n",
    "test_x,  test_y  = MNIST.testdata();\n",
    "\n",
    "test_x_vec = [vec(test_x[:, :, i]) for i = 1:10000];\n",
    "test_y_hot = [onehot(test_y[i], 0:9) for i = 1:10000];\n",
    "\n",
    "traindata = [(train_x_vec[i], train_y_hot[i]) for i = 1:60000];\n",
    "testdata = [(test_x_vec[i], test_y_hot[i]) for i = 1:10000];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train final linear layer with random standard neural network\n",
    "## Train final linear layer with random convolutional neural network\n",
    "## Hopefully the second one has a lower loss on average (If not need to rethink some things)\n",
    "## Train a standard neural network parametrized by a linear function of each layers weights\n",
    "## Evolve the linear parametrization using an evolutionary algorithm where the fitness is the average loss \n",
    "## after training of n (=10?) input parameters. Can the parameters of the linear reparameterization model\n",
    "## be sparsified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# redefine everything using Julia \"destructure\" and \"restructure\" (which is differentiable)\n",
    "# operating on the entire model rather than just individual layers\n",
    "# This may not work... but have the custom layer accept a model and a\n",
    "# reparameterization matrix as a parameter, use the reparameterized model operating\n",
    "# on x as the method (something like re(m.R * p), where re is restructure, m.R is the \n",
    "# reparameterization matrix, and p is the set of parameters returned by destructure)\n",
    "mutable struct AffineReparam\n",
    "    input_param::Vector{Float64}\n",
    "    W_param::Matrix{Float64}\n",
    "    b_param::Vector{Float64}\n",
    "    input_size::Int64\n",
    "end\n",
    "\n",
    "# Overload call, so the object can be used as a function\n",
    "# Note that if input_params will not be differentiated, the result of\n",
    "# the parameter multiplication can be stored and does not need to be constantly recomputed\n",
    "(m::AffineReparam)(x) = relu.(reshape(m.W_param * m.input_param + m.b_param, :, m.input_size) * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct AffineReparamInputCache\n",
    "    W_data::Matrix{Float64}\n",
    "    \n",
    "    prev_input_param::Vector{Float64}\n",
    "    input_param::Vector{Float64}\n",
    "    \n",
    "    W_param::Matrix{Float64}\n",
    "    b_param::Vector{Float64}\n",
    "    \n",
    "    input_size::Int64\n",
    "end\n",
    "\n",
    "AffineReparamInputCache(\n",
    "    input_param::Vector{Float64}, \n",
    "    W_param::Matrix{Float64}, \n",
    "    b_param::Vector{Float64},\n",
    "    input_size::Int64\n",
    "    ) = AffineReparamInputCache(\n",
    "        reshape(W_param * input_param + b_param, :, input_size), \n",
    "        input_param, \n",
    "        input_param, \n",
    "        W_param, \n",
    "        b_param, \n",
    "        input_size\n",
    "    )\n",
    "\n",
    "function method(x::Vector, aric::AffineReparamInputCache)\n",
    "    if (aric.prev_input_param != aric.input_param)\n",
    "        aric.W_data = reshape(aric.W_param * aric.input_param + aric.b_param, :, aric.input_size)\n",
    "        aric.prev_input_param = aric.input_param\n",
    "    end\n",
    "    return relu.(aric.W_data * x)\n",
    "end\n",
    "\n",
    "(m::AffineReparamInputCache)(x::Vector) = method(x, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct AffineReparamWbCache\n",
    "    W_data::Matrix{Float64}\n",
    "    \n",
    "    input_param::Vector{Float64}\n",
    "    \n",
    "    W_param::Matrix{Float64}\n",
    "    prev_W_param::Matrix{Float64}\n",
    "    \n",
    "    b_param::Vector{Float64}\n",
    "    prev_b_param::Vector{Float64}\n",
    "    \n",
    "    input_size::Int64\n",
    "end\n",
    "\n",
    "AffineReparamWbCache(\n",
    "    input_param::Vector{Float64}, \n",
    "    W_param::Matrix{Float64}, \n",
    "    b_param::Vector{Float64},\n",
    "    input_size::Int64\n",
    "    ) = AffineReparamWbCache(\n",
    "        reshape(W_param * input_param + b_param, :, m.input_size), \n",
    "        input_param, \n",
    "        input_param, \n",
    "        W_param,\n",
    "        W_param,\n",
    "        b_param,\n",
    "        b_param,\n",
    "        input_size\n",
    "    )\n",
    "\n",
    "# This has not been tested\n",
    "function method(x::Vector, aric::AffineReparamWbCache)\n",
    "    W_changed = aric.prev_W_param != aric.W_param\n",
    "    b_changed = aric.prev_b_param != aric.b_param\n",
    "    if (W_changed)\n",
    "        aric.W_data = reshape(aric.W_param * aric.input_param + aric.b_param, :, aric.input_size)\n",
    "        aric.prev_W_param = aric.W_param\n",
    "        aric.prev_b_param = aric.b_param\n",
    "    elseif (b_changed)\n",
    "        aric.W_data = reshape(vec(aric.W_data) - aric.prev_b_param + aric.b_param, :, aric.input_size)\n",
    "        aric.prev_b_param = aric.b_param\n",
    "    end\n",
    "    return relu.(aric.W_data * x)\n",
    "end\n",
    "\n",
    "(m::AffineReparamWbCache)(x::Vector) = method(x, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_parameters = 10;\n",
    "input = 784;\n",
    "hidden_out = 784;\n",
    "\n",
    "W_param = randn(input*hidden_out, max_parameters);\n",
    "b_param = randn(input*hidden_out);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_params1 = (1/max_parameters)*(rand(max_parameters) - 0.5*ones(max_parameters));\n",
    "input_params2 = (1/(max_parameters*hidden_out))*(rand(max_parameters) - 0.5*ones(max_parameters));\n",
    "\n",
    "hidden1 = AffineReparam(input_params1, W_param, b_param, input);\n",
    "hidden2 = AffineReparamInputCache(input_params2, W_param, b_param, input);\n",
    "\n",
    "Flux.trainable(hidden1::AffineReparamInputCache) = (hidden1.input_param, )\n",
    "Flux.trainable(hidden2::AffineReparamInputCache) = (hidden2.input_param, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([[4.621443019822122e-5, -4.098006512920417e-5, 4.321213379893864e-5, -3.74315847452869e-5, -4.679209992896548e-5, -9.17439277670436e-6, 3.44770393508489e-5, -6.284720797634103e-5, 2.4403976087266596e-5, -1.900403255369029e-5], Float32[0.08512958 0.029645827 … 0.05858114 -0.051549822; -0.05612156 -0.054823354 … 0.040251385 0.06866392; … ; -0.0019142713 -0.02679043 … -0.017604735 0.057830133; 0.0020862932 -0.001985878 … -0.055465598 -0.083531804], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_model = Chain(\n",
    "      hidden2,\n",
    "      Dense(hidden_out, 10),\n",
    "      softmax\n",
    ")\n",
    "\n",
    "num_hidden_layers = 1\n",
    "\n",
    "\n",
    "ps_hidden = Flux.params(data_model[1:num_hidden_layers])\n",
    "ps_out = Flux.params(data_model[num_hidden_layers+1:end])\n",
    "ps = Flux.params(data_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsample = sample(traindata, 6000, replace=false);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[0.019757971 -0.06254386 … -0.042520065 -0.06680834; -0.027373645 0.036480814 … 0.03772269 0.07769612; … ; 0.013659548 0.07833217 … 0.07321512 0.049626846; -0.0064494917 0.03465075 … -0.031358644 0.046776194], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.17692272 0.33308834 … 0.3083141 0.12433642; 0.38976818 -0.5449518 … -0.13594863 0.51308155; … ; 0.445382 -0.14179541 … 0.47729635 0.3308579; -0.28386572 -0.13055067 … -0.5003385 0.035294075], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_model = Chain(\n",
    "    Dense(input, hidden_out, relu),\n",
    "    Dense(hidden_out, 10),\n",
    "    softmax\n",
    ")\n",
    "ps_out = Flux.params(data_model[num_hidden_layers+1:end])\n",
    "ps = Flux.params(data_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.737920492781297\n",
      "11.292703476288668\n",
      "9.687162763882101\n",
      "8.976711613548247\n",
      "8.212386323411408\n",
      "7.662491690856086\n",
      "7.133717443827318\n",
      "6.946730588710791\n",
      "6.748966088249661\n",
      "6.676684505510985\n",
      "6.442453409085176\n",
      "6.436664022720092\n",
      "6.14534293009462\n",
      "5.981560920160931\n",
      "5.716034799171579\n",
      "5.619660653357848\n",
      "5.60205436060706\n",
      "5.55411420492969\n",
      "5.407255763298242\n",
      "5.258720546464656\n",
      "5.1143038645518395\n",
      "5.063784894200569\n",
      "4.984624834641118\n",
      "4.920884895694458\n",
      "4.849307204589918\n",
      "4.758002899784046\n",
      "4.697330896490663\n",
      "4.6712431141644455\n",
      "4.638297400090243\n",
      "4.564294478198112\n",
      "4.519527232731251\n",
      "4.434911797064934\n",
      "4.386404096544667\n",
      "4.3747374347035635\n",
      "4.324686194850671\n",
      "4.306626746313406\n",
      "4.2785413258261045\n",
      "4.256428926563204\n",
      "4.213508861671226\n",
      "4.213840574720204\n",
      "4.168278752848328\n",
      "4.120812441340383\n",
      "4.142371699890549\n",
      "4.119482964598424\n",
      "4.094899818521346\n",
      "4.061786627986874\n",
      "4.034886211578239\n",
      "3.999136116449572\n",
      "3.9592355422799086\n",
      "3.9371193897402277\n",
      "3.9044702020109754\n",
      "3.876623150586507\n",
      "3.8464911223410585\n",
      "3.825075802133967\n",
      "3.811055239224025\n",
      "3.829732371600751\n",
      "3.812973897785329\n",
      "3.810678387216541\n",
      "3.819448077387994\n",
      "3.823345459655324\n",
      "1"
     ]
    }
   ],
   "source": [
    "loss(x, y) = Flux.Losses.crossentropy(data_model(x), y)\n",
    "opt = Descent(.001)\n",
    "\n",
    "epochs = 1\n",
    "for epoch in 1:epochs\n",
    "    loss_total = 0\n",
    "    for (i, d) in enumerate(trainsample)\n",
    "        diff_ps = ps_out\n",
    "        gs = gradient(diff_ps) do\n",
    "          training_loss = loss(d...)\n",
    "          # Code inserted here will be differentiated, unless you need that gradient information\n",
    "          # it is better to do the work outside this block.\n",
    "          return training_loss\n",
    "        end\n",
    "        loss_total += loss(d...)\n",
    "        if i%100==0\n",
    "            println(loss_total/i)\n",
    "        end\n",
    "        # Insert whatever code you want here that needs training_loss, e.g. logging.\n",
    "        # logging_callback(training_loss)\n",
    "        # Insert what ever code you want here that needs gradient.\n",
    "        # E.g. logging with TensorBoardLogger.jl as histogram so you can see if it is becoming huge.\n",
    "        update!(opt, diff_ps, gs)\n",
    "    end\n",
    "    print(epoch)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([[4.621443019822122e-5, -4.098006512920417e-5, 4.321213379893864e-5, -3.74315847452869e-5, -4.679209992896548e-5, -9.17439277670436e-6, 3.44770393508489e-5, -6.284720797634103e-5, 2.4403976087266596e-5, -1.900403255369029e-5]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(x, y) = mean(onecold.(x) .== onecold.(y))\n",
    "#accuracy(m.(train_x_vec), train_y_hot)\n",
    "ps_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8418"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(data_model.(test_x_vec), test_y_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
