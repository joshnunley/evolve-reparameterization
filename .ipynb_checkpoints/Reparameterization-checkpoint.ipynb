{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d60218e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, MLDatasets, ImageCore, StatsBase\n",
    "using MLDatasets: MNIST\n",
    "using Flux: train!, update!\n",
    "using Flux: onehot, throttle, crossentropy\n",
    "using StatsBase: sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76bad379",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux: Data.DataLoader\n",
    "using Flux: onehotbatch, onecold, crossentropy\n",
    "using Flux: @epochs\n",
    "using Statistics\n",
    "using MLDatasets\n",
    "\n",
    "# load full training set\n",
    "train_x, train_y = MNIST.traindata();\n",
    "\n",
    "train_x_vec = [vec(train_x[:, :, i]) for i = 1:60000];\n",
    "train_y_hot = [onehot(train_y[i], 0:9) for i = 1:60000];\n",
    "\n",
    "# load full test set\n",
    "test_x,  test_y  = MNIST.testdata();\n",
    "\n",
    "test_x_vec = [vec(test_x[:, :, i]) for i = 1:10000];\n",
    "test_y_hot = [onehot(test_y[i], 0:9) for i = 1:10000];\n",
    "\n",
    "traindata = [(train_x_vec[i], train_y_hot[i]) for i = 1:60000];\n",
    "testdata = [(test_x_vec[i], test_y_hot[i]) for i = 1:10000];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4972d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train final linear layer with random standard neural network\n",
    "## Train final linear layer with random convolutional neural network\n",
    "## Hopefully the second one has a lower loss on average (If not need to rethink some things)\n",
    "## Train a standard neural network parametrized by a linear function of each layers weights\n",
    "## Evolve the linear parametrization using an evolutionary algorithm where the fitness is the average loss \n",
    "## after training of n (=10?) input parameters. Can the parameters of the linear reparameterization model\n",
    "## be sparsified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e4dc0c34",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching Dense(::Vector{Float64}, ::Vector{Float64}, ::typeof(relu))\n\u001b[0mClosest candidates are:\n\u001b[0m  Dense(\u001b[91m::M\u001b[39m, ::Any, ::F) where {M<:(AbstractMatrix{T} where T), F} at /Users/joshnunley/.julia/packages/Flux/ZnXxS/src/layers/basic.jl:127\n\u001b[0m  Dense(\u001b[91m::Integer\u001b[39m, \u001b[91m::Integer\u001b[39m, ::Any; initW, initb, init, bias) at /Users/joshnunley/.julia/packages/Flux/ZnXxS/src/layers/basic.jl:133\n\u001b[0m  Dense(\u001b[91m::M\u001b[39m, ::Any) where M<:(AbstractMatrix{T} where T) at /Users/joshnunley/.julia/packages/Flux/ZnXxS/src/layers/basic.jl:127",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching Dense(::Vector{Float64}, ::Vector{Float64}, ::typeof(relu))\n\u001b[0mClosest candidates are:\n\u001b[0m  Dense(\u001b[91m::M\u001b[39m, ::Any, ::F) where {M<:(AbstractMatrix{T} where T), F} at /Users/joshnunley/.julia/packages/Flux/ZnXxS/src/layers/basic.jl:127\n\u001b[0m  Dense(\u001b[91m::Integer\u001b[39m, \u001b[91m::Integer\u001b[39m, ::Any; initW, initb, init, bias) at /Users/joshnunley/.julia/packages/Flux/ZnXxS/src/layers/basic.jl:133\n\u001b[0m  Dense(\u001b[91m::M\u001b[39m, ::Any) where M<:(AbstractMatrix{T} where T) at /Users/joshnunley/.julia/packages/Flux/ZnXxS/src/layers/basic.jl:127",
      "",
      "Stacktrace:",
      " [1] Dense(in::Int64, out::Int64, Ïƒ::Function; initW::typeof(W_init), initb::typeof(b_init), init::typeof(Flux.glorot_uniform), bias::Bool)",
      "   @ Flux ~/.julia/packages/Flux/ZnXxS/src/layers/basic.jl:151",
      " [2] top-level scope",
      "   @ In[83]:22",
      " [3] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [4] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1094"
     ]
    }
   ],
   "source": [
    "parameter_model = function(x, W = nothing, b = nothing)\n",
    "    W*x + b;\n",
    "end\n",
    "\n",
    "max_parameters = 9\n",
    "input = 784\n",
    "hidden_out = 32\n",
    "\n",
    "W_weight = randn(input*hidden_out, max_parameters)\n",
    "b_weight = randn(input*hidden_out)\n",
    "#W_bias = randn(hidden_out, max_parameters)\n",
    "#b_bias = randn(hidden_out)\n",
    "\n",
    "weight_model(x) = parameter_model(x, W_weight, b_weight)\n",
    "#bias_model(x) = parameter_model(x, W_bias, b_bias)\n",
    "\n",
    "x = randn(max_parameters)\n",
    "\n",
    "W_init(out, in) = weight_model(x)\n",
    "b_init(out) = zeros(out)\n",
    "\n",
    "data_model = Chain(\n",
    "      Dense(input, hidden_out, relu; initW=W_init, initb=b_init),\n",
    "      Dense(hidden_out, 10),\n",
    "      softmax\n",
    "    )\n",
    "\n",
    "num_hidden_layers = 1\n",
    "ps_hidden = Flux.params(m[1:num_hidden_layers])\n",
    "\n",
    "#ps_out = Flux.params(m[2:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3ef008a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::Flux.var\"#throttled#68\"{Flux.var\"#throttled#64#69\"{Bool, Bool, typeof(evalcb), Int64}}) (generic function with 1 method)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_init(out) = zeros(out)\n",
    "\n",
    "m = Chain(\n",
    "      Dense(784, 784, relu; initb=b_init),\n",
    "      Dense(784, 10),\n",
    "      softmax\n",
    "    )\n",
    "ps_out = Flux.params(m[2:end])\n",
    "\n",
    "loss(x, y) = Flux.Losses.crossentropy(m(x), y)\n",
    "\n",
    "opt = Descent(0.1)\n",
    "\n",
    "accuracy(x, y) = mean(onecold.(x) .== onecold.(y))\n",
    "evalcb() = @show(loss(train, test_y))\n",
    "throttled_cb = throttle(evalcb, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2a29516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsample = sample(traindata, 600, replace=false);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "222ac940",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, d) in enumerate(trainsample)\n",
    "    gs = gradient(ps_out) do\n",
    "      training_loss = loss(d...)\n",
    "      # Code inserted here will be differentiated, unless you need that gradient information\n",
    "      # it is better to do the work outside this block.\n",
    "      return training_loss\n",
    "    end\n",
    "    # Insert whatever code you want here that needs training_loss, e.g. logging.\n",
    "    # logging_callback(training_loss)\n",
    "    # Insert what ever code you want here that needs gradient.\n",
    "    # E.g. logging with TensorBoardLogger.jl as histogram so you can see if it is becoming huge.\n",
    "    update!(opt, ps_out, gs)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f8e0fcca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7251333333333333"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(m.(train_x_vec), train_y_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a0a10666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7641"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(m.(test_x_vec), test_y_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af09aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
